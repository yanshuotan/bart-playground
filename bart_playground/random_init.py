import numpy as np
import math
from bart_playground.params import Tree
from bart_playground.util import DefaultPreprocessor, fast_choice
from bart_playground.priors import TreesPrior
from scipy.stats import invgamma
        
def generate_data_from_defaultbart_prior(
    X: np.ndarray,
    *,
    n_trees: int = 200,
    tree_alpha: float = 0.95,
    tree_beta: float = 2.0,
    f_k: float = 2.0,
    eps_nu: float = 3.0,
    eps_lambda: float = 1.0,
    eps_sigma2: float | None = None,
    random_state: int | None = 42,
    max_depth: int = 10,
    min_node_size: int = 1,
    quick_decay: bool = False,
    max_bins: int = 100,
    return_latent: bool = True,
):
    """
    Generate regression data according to the DefaultBART prior.

    Generation process (prior predictive distribution):
    - For each tree, recursively sample the tree structure according to the prior p(split|depth)=alpha/(1+depth)^beta (quick_decay=False);
      variables are chosen uniformly, thresholds are sampled from the candidate set generated by the preprocessor; leaf values are drawn from N(0, f_sigma2), where f_sigma2=0.25/(f_k^2*n_trees)
    - f(x) is the sum of outputs from all trees
    - Noise variance sigma^2 ~ InvGamma(eps_nu/2, scale=eps_nu*eps_lambda/2), y = f(x) + eps

    Parameter notes:
    - eps_lambda is the noise prior scale (cannot be adaptively calibrated without data, must be set manually; default is 1.0)
    - max_depth/min_node_size limit the size of generated trees to avoid degenerate splits

    Returns:
    - (X, y); if return_latent=True, returns (X, y, f, sigma2, trees)
    """
    # Get dimensions and RNG
    n = X.shape[0]
    p = X.shape[1]
    rng = np.random.default_rng(random_state)

    # Candidate thresholds via DefaultPreprocessor (same mechanism DefaultBART uses)
    preprocessor = DefaultPreprocessor(max_bins=max_bins)
    thresholds_by_var = preprocessor.gen_thresholds(X)

    # Prior container for consistency with DefaultBART (for f_sigma2)
    tree_prior = TreesPrior(n_trees=n_trees, tree_alpha=tree_alpha, tree_beta=tree_beta, f_k=f_k, generator=rng, quick_decay=quick_decay)

    def p_split(depth: int) -> float:
        if quick_decay:
            return float(np.clip(tree_alpha ** depth, 0.0, 1.0))
        return float(np.clip(tree_alpha / ((1.0 + depth) ** tree_beta), 0.0, 1.0))

    def sample_tree_outputs_one_tree() -> tuple[np.ndarray, Tree]:
        # Start with a stump bound to X so internal counts/caches update on splits
        t = Tree.new(dataX=X)

        # Maintain a queue of (node_id, depth) for leaf nodes to consider splitting
        queue = [(0, 0)]
        while queue:
            node_id, depth = queue.pop()
            # Only attempt split if still a leaf and under constraints
            if depth >= max_depth:
                continue
            if t.vars[node_id] != -1:
                continue  # already split

            # Split decision under prior
            if rng.uniform() > p_split(depth):
                continue

            # Try a few times to find a valid (var, threshold) that yields non-empty children
            success = False
            for _ in range(32):
                var = int(fast_choice(rng, np.arange(p)))
                cands = thresholds_by_var[var]
                if cands.size == 0:
                    continue
                thr = fast_choice(rng, cands)
                is_valid = t.split_leaf(node_id=node_id, var=var, threshold=thr)
                if is_valid and (t.n[node_id * 2 + 1] >= min_node_size) and (t.n[node_id * 2 + 2] >= min_node_size):
                    success = True
                    break
                # revert failed split (prune back if created) â€“ if invalid, the helper didn't finalize counts,
                # but to be safe, ensure node remains a leaf by pruning.
                if t.vars[node_id] != -1:
                    try:
                        t.prune_split(node_id)
                    except Exception:
                        pass
            if not success:
                continue

            # Enqueue children
            left = node_id * 2 + 1
            right = node_id * 2 + 2
            queue.append((left, depth + 1))
            queue.append((right, depth + 1))

        # Draw leaf values ~ N(0, f_sigma2)
        f_sigma2 = tree_prior.f_sigma2
        leaf_ids = t.leaves
        t.leaf_vals[leaf_ids] = rng.normal(0.0, np.sqrt(f_sigma2), size=leaf_ids.size).astype(t.float_dtype)
        t.update_outputs()
        return t.evals.astype(np.float32), t

    # 2) Sum of trees
    f = np.zeros(n, dtype=np.float32)
    trees = []
    for _ in range(n_trees):
        f_tree, t = sample_tree_outputs_one_tree()
        f += f_tree
        if return_latent:
            trees.append(t)

    # 3) Noise variance prior and observations
    # sigma^2 ~ InvGamma(eps_nu/2, scale = eps_nu * eps_lambda / 2)
    sigma2 = float(invgamma.rvs(a=eps_nu / 2.0, scale=eps_nu * eps_lambda / 2.0, random_state=rng)) if eps_sigma2 is None else eps_sigma2
    y = f + rng.normal(0.0, np.sqrt(sigma2), size=n).astype(np.float32)

    if return_latent:
        return X, y, f, sigma2, trees
    return X, y