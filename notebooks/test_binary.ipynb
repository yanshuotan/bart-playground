{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test BinaryBART\n",
        "<sup>*</sup>Including ProbitBART and LogisticBART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.datasets import make_classification, load_breast_cancer, load_wine, fetch_openml\n",
        "import pandas as pd\n",
        "from bart_playground import *\n",
        "from bart_playground.bart import DefaultBART, ProbitBART\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "import bartz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "N_TREES = 50\n",
        "NDPOST = 1000\n",
        "NSKIP = 200\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# If debug then run with only one dataset and record running time\n",
        "# Otherwise run with all datasets\n",
        "debug = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder, normalize\n",
        "\n",
        "def load_mushroom():\n",
        "    X, y = fetch_openml('mushroom', version=1, return_X_y=True)\n",
        "    for col in X.select_dtypes('category'):\n",
        "        # -1 in codes indicates NaN by pandas convention\n",
        "        X[col] = X[col].cat.codes\n",
        "    X = normalize(X)\n",
        "    y_array = y.to_numpy().reshape(-1, 1)\n",
        "    y_arm = OrdinalEncoder(dtype=int).fit_transform(y_array).flatten()\n",
        "    \n",
        "    # make the dataset a little bit smaller\n",
        "    indices = np.random.choice(X.shape[0], size=2000, replace=False)\n",
        "    X = X[indices, :]\n",
        "    y_arm = y_arm[indices]\n",
        "    \n",
        "    return X, y_arm\n",
        "\n",
        "def load_mushroom_encoded(X, y_arm):\n",
        "    n_arm = np.max(y_arm) + 1\n",
        "    dim = X.shape[1] * n_arm # total number of encoded covariates (location-encoded for each arm) \n",
        "    act_dim = X.shape[1] # number of covariates\n",
        "    covariates = np.zeros((X.shape[0], dim))\n",
        "    rewards = np.zeros((X.shape[0], ))\n",
        "    for cursor in range(X.shape[0]):\n",
        "        a = np.random.randint(0, n_arm)\n",
        "        covariates[cursor, a * act_dim:(a * act_dim + act_dim)] = X[cursor]\n",
        "        if y_arm[cursor] == a:\n",
        "            rewards[cursor] = 1 # reward is 1 if the true category matches the chosen arm\n",
        "\n",
        "    return covariates, rewards\n",
        "\n",
        "def load_mushroom_encoded_hot(X, y_arm):\n",
        "    n_arm = np.max(y_arm) + 1\n",
        "    dim = X.shape[1] + n_arm # total number of encoded covariates (onehot-encoded for each arm) \n",
        "    act_dim = X.shape[1] # number of covariates\n",
        "    covariates = np.zeros((X.shape[0], dim))\n",
        "    rewards = np.zeros((X.shape[0], ))\n",
        "    for cursor in range(X.shape[0]):\n",
        "        a = np.random.randint(0, n_arm)\n",
        "        covariates[cursor, :act_dim] = X[cursor]\n",
        "        covariates[cursor, act_dim + a] = 1 # one-hot encoding\n",
        "        if y_arm[cursor] == a:\n",
        "            rewards[cursor] = 1 # reward is 1 if the true category matches the chosen arm\n",
        "\n",
        "    return covariates, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "def load_datasets():\n",
        "    # Synthetic dataset\n",
        "    X_syn, y_syn = make_classification(n_samples=400, n_features=8, n_informative=6, \n",
        "                                       n_redundant=0, n_classes=2, random_state=RANDOM_STATE)\n",
        "    \n",
        "    # Breast cancer dataset\n",
        "    X_bc, y_bc = load_breast_cancer(return_X_y=True)\n",
        "    \n",
        "    # Wine dataset (convert to binary: class 0 vs rest)\n",
        "    X_wine, y_wine = load_wine(return_X_y=True)\n",
        "    y_wine = (y_wine == 0).astype(int)\n",
        "    \n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    \n",
        "    X_mushroom, y_mushroom = load_mushroom()\n",
        "    X_mr_encoded, y_mr_encoded = load_mushroom_encoded(X_mushroom, y_mushroom)\n",
        "    \n",
        "    return {\n",
        "        # \"Synthetic\": (X_syn, y_syn),\n",
        "        # \"Breast Cancer\": (X_bc, y_bc),\n",
        "        # \"Wine Binary\": (X_wine, y_wine),\n",
        "        # \"Mushroom\": (X_mushroom, y_mushroom),\n",
        "        # \"Mushroom Encoded\": (X_mr_encoded, y_mr_encoded),\n",
        "        \"Mushroom Encoded Hot\": load_mushroom_encoded_hot(X_mushroom, y_mushroom),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility functions to evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ece_score(prob_true, prob_pred, y_true, y_prob, n_bins):\n",
        "    # Compute bin counts for weighting\n",
        "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    bin_ids = np.digitize(y_prob, bin_edges) - 1\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        mask = bin_ids == i\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        # weight by fraction of samples in bin\n",
        "        weight = mask.sum() / len(y_true)\n",
        "        ece += weight * abs(prob_true[i] - prob_pred[i])\n",
        "    return ece\n",
        "\n",
        "def mce_score(prob_true, prob_pred):\n",
        "    diffs = np.abs(prob_true - prob_pred)\n",
        "    return np.max(diffs)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calibration_plot(prob_true, prob_pred, model_name, dataset_name):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(prob_pred, prob_true, marker='o', label='Calibration curve')\n",
        "    plt.plot([0,1], [0,1], linestyle='--', label='Perfect calibration')\n",
        "    plt.xlabel('Mean predicted probability')\n",
        "    plt.ylabel('Fraction of positives')\n",
        "    plt.title(f'Reliability Diagram ({dataset_name}, {model_name})')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'./results/{dataset_name}_{model_name}_calibration.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import nan\n",
        "\n",
        "def evaluate_model(model, model_name, X_train, X_test, y_train, y_test, dataset_name):\n",
        "    \"\"\"Evaluate a single model and return metrics\"\"\"\n",
        "    if model_name == \"Bartz\":\n",
        "        # Bartz regression treating 0/1 as continuous\n",
        "        fit_result = bartz.BART.gbart(\n",
        "            x_train=X_train.T, y_train=y_train.astype(float),\n",
        "            x_test=X_test.T,\n",
        "            ntree=N_TREES, ndpost=NDPOST, nskip=NSKIP,\n",
        "            seed=RANDOM_STATE,\n",
        "            printevery=NDPOST + NSKIP + 100\n",
        "        )\n",
        "        btpred_all = fit_result.predict(np.transpose(X_test))\n",
        "        btpred = np.mean(np.array(btpred_all), axis=0)\n",
        "        y_pred_prob = np.clip(btpred, 1e-9, 1 - 1e-9)\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "        \n",
        "    elif model_name == \"ProbitBART\" or model_name == \"LogisticBART\":\n",
        "        # Proper binary BART\n",
        "        model.fit(X_train, y_train)\n",
        "        proba_output = model.predict_proba(X_test)\n",
        "        y_pred_prob = proba_output[:, 1]\n",
        "        y_pred = np.argmax(proba_output, axis=1)\n",
        "        \n",
        "        # y_post = model.posterior_predict(X_test)\n",
        "        # lower_ci = np.percentile(y_post, 2.5, axis=1)\n",
        "        # upper_ci = np.percentile(y_post, 97.5, axis=1)\n",
        "        # coverage = np.mean((y_test >= lower_ci) & (y_test <= upper_ci))\n",
        "\n",
        "    elif model_name == \"RandomForestClassifier\":\n",
        "        # Native binary classifier\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "    else:\n",
        "        # Regression methods treating 0/1 as continuous\n",
        "        model.fit(X_train, y_train)\n",
        "        raw_pred = model.predict(X_test)\n",
        "        y_pred_prob = np.clip(raw_pred, 1e-9, 1 - 1e-9)\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    logloss = log_loss(y_test, y_pred_prob)\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    \n",
        "    n_bins = 5\n",
        "    prob_true, prob_pred = calibration_curve(y_test, y_pred_prob, n_bins=n_bins, strategy='uniform')\n",
        "    if(len(prob_true) < n_bins):\n",
        "        ece = nan\n",
        "        mce = nan\n",
        "    else:\n",
        "        ece = ece_score(prob_true, prob_pred, y_test, y_pred_prob, n_bins=n_bins)\n",
        "        mce = mce_score(prob_true, prob_pred)\n",
        "        calibration_plot(prob_true, prob_pred, model_name, dataset_name)\n",
        "\n",
        "    return {'Accuracy': accuracy, 'LogLoss': logloss, 'AUC': auc,\n",
        "            'ECE': ece, 'MCE': mce}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = None\n",
        "results = []\n",
        " \n",
        "def record_evaluation_results(dataset_name, X, y):\n",
        "    global metrics\n",
        "    \n",
        "    print(f\"\\n=== Testing on {dataset_name} ===\")\n",
        "    \n",
        "    if dataset_name == \"Mushroom\" or dataset_name == \"Mushroom Encoded\":\n",
        "        test_size = 0.3\n",
        "    else:\n",
        "        test_size = 0.3\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Define models\n",
        "    models = {\n",
        "        \"XGBClassifier\": XGBClassifier(n_estimators=N_TREES, random_state=RANDOM_STATE, eval_metric='logloss'),\n",
        "        \"XGBRegressor\": XGBRegressor(n_estimators=N_TREES, random_state=RANDOM_STATE, eval_metric='logloss'),\n",
        "        \"RFClassifier\": RandomForestClassifier(n_estimators=N_TREES, random_state=RANDOM_STATE),\n",
        "        \"RFRegressor\": RandomForestRegressor(n_estimators=N_TREES, random_state=RANDOM_STATE),\n",
        "        \"Bartz\": \"placeholder\",\n",
        "        \"DefaultBART\": DefaultBART(n_trees=N_TREES, ndpost=NDPOST, nskip=NSKIP, random_state=RANDOM_STATE),\n",
        "        # \"ProbitBART\": ProbitBART(n_trees=N_TREES, ndpost=NDPOST, nskip=NSKIP, random_state=RANDOM_STATE),\n",
        "        \"LogisticBART\": LogisticBART(n_trees=N_TREES, ndpost=NDPOST, nskip=NSKIP, random_state=RANDOM_STATE)\n",
        "    }\n",
        "    \n",
        "    for model_name, model in models.items():\n",
        "        print(f\"  Training {model_name}...\")\n",
        "        \n",
        "        X_tr, X_te = X_train, X_test\n",
        "        \n",
        "        metrics = evaluate_model(model, model_name, X_tr, X_te, y_train, y_test, dataset_name)\n",
        "        \n",
        "        result = {'Dataset': dataset_name, 'Model': model_name, **metrics}\n",
        "        results.append(result)\n",
        "        print(f\"    Acc: {metrics['Accuracy']:.3f}, LogLoss: {metrics['LogLoss']:.3f}, AUC: {metrics['AUC']:.4f}\")\n",
        "        print(f\"    ECE: {metrics['ECE']:.4f}, MCE: {metrics['MCE']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bart_playground.bart import LogisticBART\n",
        "\n",
        "old_settings = np.seterr(invalid='raise')\n",
        "\n",
        "datasets = load_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: Mushroom Encoded Hot\n",
            "X shape: (2000, 24), y shape: (2000,)\n",
            "y distribution: {0.0: 0.5045, 1.0: 0.4955}\n"
          ]
        }
      ],
      "source": [
        "for name, (X, y) in datasets.items():\n",
        "    # Print dataset shapes\n",
        "    print(f\"Dataset: {name}\\nX shape: {X.shape}, y shape: {y.shape}\")\n",
        "    # Print 0-1 distribution of y\n",
        "    print(f\"y distribution: {pd.Series(y).value_counts(normalize=True).to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not debug:\n",
        "    for dataset_name, (X, y) in list(datasets.items()):\n",
        "        record_evaluation_results(dataset_name, X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing on Mushroom Encoded Hot ===\n",
            "  Training XGBClassifier...\n",
            "    Acc: 0.970, LogLoss: 0.622, AUC: 0.9699\n",
            "    ECE: nan, MCE: nan\n",
            "  Training XGBRegressor...\n",
            "    Acc: 0.968, LogLoss: 0.141, AUC: 0.9899\n",
            "    ECE: 0.0152, MCE: 0.1041\n",
            "  Training RFClassifier...\n",
            "    Acc: 0.973, LogLoss: 0.553, AUC: 0.9732\n",
            "    ECE: nan, MCE: nan\n",
            "  Training RFRegressor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:2025-07-07 11:14:15,776:jax._src.xla_bridge:867: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
            "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Acc: 0.982, LogLoss: 0.112, AUC: 0.9989\n",
            "    ECE: 0.0751, MCE: 0.2422\n",
            "  Training Bartz...\n",
            "    Acc: 0.998, LogLoss: 0.029, AUC: 0.9999\n",
            "    ECE: 0.0183, MCE: 0.3060\n",
            "  Training DefaultBART...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterations: 100%|██████████| 1200/1200 [00:06<00:00, 199.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Acc: 0.985, LogLoss: 0.077, AUC: 0.9977\n",
            "    ECE: 0.0403, MCE: 0.3252\n",
            "  Training LogisticBART...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterations: 100%|██████████| 1200/1200 [00:11<00:00, 104.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Acc: 0.922, LogLoss: 0.213, AUC: 0.9768\n",
            "    ECE: 0.0560, MCE: 0.1184\n"
          ]
        }
      ],
      "source": [
        "if debug == True:\n",
        "    dataset_name, (X, y) = list(datasets.items())[-1]  # Last dataset for debugging\n",
        "    \n",
        "    profile = False\n",
        "    if not profile:\n",
        "        record_evaluation_results(dataset_name, X, y)\n",
        "    else:\n",
        "        %prun -s cumtime -D temp_profile.prof -q record_evaluation_results(dataset_name, X, y)\n",
        "\n",
        "        fname = \"profile_logisticbart\"\n",
        "\n",
        "        !mv temp_profile.prof {fname}.prof\n",
        "        !gprof2dot -f pstats {fname}.prof -o {fname}.dot\n",
        "        !dot -Tpng {fname}.dot -o {fname}.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SUMMARY RESULTS\n",
            "============================================================\n",
            "\n",
            "Accuracy:\n",
            "Model                 Bartz  DefaultBART  LogisticBART  RFClassifier  \\\n",
            "Dataset                                                                \n",
            "Mushroom Encoded Hot  0.998        0.985         0.922         0.973   \n",
            "\n",
            "Model                 RFRegressor  XGBClassifier  XGBRegressor  \n",
            "Dataset                                                         \n",
            "Mushroom Encoded Hot        0.982           0.97         0.968  \n",
            "\n",
            "AUC:\n",
            "Model                 Bartz  DefaultBART  LogisticBART  RFClassifier  \\\n",
            "Dataset                                                                \n",
            "Mushroom Encoded Hot    1.0        0.998         0.977         0.973   \n",
            "\n",
            "Model                 RFRegressor  XGBClassifier  XGBRegressor  \n",
            "Dataset                                                         \n",
            "Mushroom Encoded Hot        0.999           0.97          0.99  \n",
            "\n",
            "LogLoss:\n",
            "Model                 Bartz  DefaultBART  LogisticBART  RFClassifier  \\\n",
            "Dataset                                                                \n",
            "Mushroom Encoded Hot  0.029        0.077         0.213         0.553   \n",
            "\n",
            "Model                 RFRegressor  XGBClassifier  XGBRegressor  \n",
            "Dataset                                                         \n",
            "Mushroom Encoded Hot        0.112          0.622         0.141  \n",
            "\n",
            "ECE:\n",
            "Model                 Bartz  DefaultBART  LogisticBART  RFRegressor  \\\n",
            "Dataset                                                               \n",
            "Mushroom Encoded Hot  0.018         0.04         0.056        0.075   \n",
            "\n",
            "Model                 XGBRegressor  \n",
            "Dataset                             \n",
            "Mushroom Encoded Hot         0.015  \n",
            "\n",
            "MCE:\n",
            "Model                 Bartz  DefaultBART  LogisticBART  RFRegressor  \\\n",
            "Dataset                                                               \n",
            "Mushroom Encoded Hot  0.306        0.325         0.118        0.242   \n",
            "\n",
            "Model                 XGBRegressor  \n",
            "Dataset                             \n",
            "Mushroom Encoded Hot         0.104  \n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pivot tables for easy comparison\n",
        "for metric in ['Accuracy', 'AUC', 'LogLoss', 'ECE', 'MCE']:\n",
        "    print(f\"\\n{metric}:\")\n",
        "    pivot = results_df.pivot_table(index='Dataset', columns='Model', values=metric)\n",
        "    print(pivot.round(3))"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "bartpg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
