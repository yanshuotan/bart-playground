{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_playground import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook = \"real3_CalHousing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values.astype(float)\n",
    "y = np.array(y).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndpost = 50000\n",
    "nskip = 0\n",
    "n_trees = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 95 concurrent workers.\n",
      "Iterations: 100%|██████████| 50000/50000 [53:45<00:00, 15.50it/s]   \n",
      "Iterations: 100%|██████████| 50000/50000 [54:22<00:00, 15.33it/s]\n",
      "Iterations: 100%|██████████| 50000/50000 [54:24<00:00, 15.32it/s]\n",
      "Iterations: 100%|██████████| 50000/50000 [54:44<00:00, 15.22it/s]\n",
      "Iterations: 100%|██████████| 50000/50000 [55:20<00:00, 15.06it/s]\n",
      "Iterations:   2%|▏         | 854/50000 [09:50<11:02:14,  1.24it/s] "
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}\nDetailed tracebacks of the workers should have been printed to stderr in the executor process if faulthandler was not disabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTerminatedWorkerError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexperiment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_parallel_experiments\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Run 5 parallel experiments\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43mrun_parallel_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndpost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnskip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/home/svu/e0950116/bart-playground/diagnosis/experiment.py:112\u001b[39m, in \u001b[36mrun_parallel_experiments\u001b[39m\u001b[34m(X, y, ndpost, nskip, n_trees, notebook, tree_alpha, tree_beta, m_tries, n_runs, n_jobs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_parallel_experiments\u001b[39m(X, y, ndpost, nskip, n_trees, notebook, tree_alpha=\u001b[32m0.95\u001b[39m, tree_beta=\u001b[32m2.0\u001b[39m, m_tries=\u001b[32m10\u001b[39m, n_runs=\u001b[32m5\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run parallel experiments with different train-test splits\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndpost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnskip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_tries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_beta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Combine all results into structured arrays\u001b[39;00m\n\u001b[32m    118\u001b[39m     combined_results = {\n\u001b[32m    119\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    120\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msigmas\u001b[39m\u001b[33m'\u001b[39m: np.array([r[\u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33msigmas\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]),\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m         }\n\u001b[32m    140\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bart-playground/venv/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bart-playground/venv/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bart-playground/venv/lib/python3.11/site-packages/joblib/parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bart-playground/venv/lib/python3.11/site-packages/joblib/parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bart-playground/venv/lib/python3.11/site-packages/joblib/parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bart-playground/venv/lib/python3.11/site-packages/joblib/parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mTerminatedWorkerError\u001b[39m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}\nDetailed tracebacks of the workers should have been printed to stderr in the executor process if faulthandler was not disabled."
     ]
    }
   ],
   "source": [
    "from experiment import run_parallel_experiments\n",
    "\n",
    "# Run 5 parallel experiments\n",
    "results = run_parallel_experiments(X, y, ndpost, nskip, n_trees, notebook, n_runs=5, n_jobs=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = np.load(f'store/{notebook}.npz', allow_pickle=True)\n",
    "\n",
    "default_data = experiment_results['default'].item()\n",
    "mtmh_data = experiment_results['mtmh'].item()\n",
    "metadata = experiment_results['metadata'].item()\n",
    "\n",
    "n_runs = metadata['n_runs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Plots Analysis\n",
    "## Tree Depth / #Leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract leaves and depths data for visualization\n",
    "n_runs = experiment_results['metadata'].item()['n_runs']\n",
    "\n",
    "fig, axes = plt.subplots(n_runs, 2, figsize=(15, 4*n_runs))\n",
    "if n_runs == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    # Extract data for this run\n",
    "    default_leaves = experiment_results['default'].item()['leaves'][run_id]\n",
    "    mtmh_leaves = experiment_results['mtmh'].item()['leaves'][run_id]\n",
    "    default_depths = experiment_results['default'].item()['depths'][run_id]\n",
    "    mtmh_depths = experiment_results['mtmh'].item()['depths'][run_id]\n",
    "    \n",
    "    # Plot leaves (left column)\n",
    "    axes[run_id, 0].plot(default_leaves, label='Default BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 0].plot(mtmh_leaves, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 0].set_title(f'Run {run_id} - Average Leaves per Tree')\n",
    "    axes[run_id, 0].set_xlabel('Iteration')\n",
    "    axes[run_id, 0].set_ylabel('Average Leaves')\n",
    "    axes[run_id, 0].legend()\n",
    "    axes[run_id, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot depths (right column)\n",
    "    axes[run_id, 1].plot(default_depths, label='Default BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 1].plot(mtmh_depths, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 1].set_title(f'Run {run_id} - Average Tree Depth')\n",
    "    axes[run_id, 1].set_xlabel('Iteration')\n",
    "    axes[run_id, 1].set_ylabel('Average Depth')\n",
    "    axes[run_id, 1].legend()\n",
    "    axes[run_id, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract data across all runs\n",
    "n_runs = experiment_results['metadata'].item()['n_runs']\n",
    "\n",
    "# Collect all runs data\n",
    "default_leaves_all = []\n",
    "mtmh_leaves_all = []\n",
    "default_depths_all = []\n",
    "mtmh_depths_all = []\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    default_leaves_all.append(experiment_results['default'].item()['leaves'][run_id])\n",
    "    mtmh_leaves_all.append(experiment_results['mtmh'].item()['leaves'][run_id])\n",
    "    default_depths_all.append(experiment_results['default'].item()['depths'][run_id])\n",
    "    mtmh_depths_all.append(experiment_results['mtmh'].item()['depths'][run_id])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "default_leaves_all = np.array(default_leaves_all)  # Shape: (n_runs, n_iterations)\n",
    "mtmh_leaves_all = np.array(mtmh_leaves_all)\n",
    "default_depths_all = np.array(default_depths_all)\n",
    "mtmh_depths_all = np.array(mtmh_depths_all)\n",
    "\n",
    "# Calculate mean and std across runs\n",
    "default_leaves_mean = np.mean(default_leaves_all, axis=0)\n",
    "mtmh_leaves_mean = np.mean(mtmh_leaves_all, axis=0)\n",
    "\n",
    "default_depths_mean = np.mean(default_depths_all, axis=0)\n",
    "mtmh_depths_mean = np.mean(mtmh_depths_all, axis=0)\n",
    "\n",
    "# Create iteration axis\n",
    "iterations = np.arange(len(default_leaves_mean))\n",
    "\n",
    "# Plot averaged results with error bands\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot leaves\n",
    "ax1.plot(iterations, default_leaves_mean, label='Default BART', alpha=0.7, linewidth=1)\n",
    "ax1.plot(iterations, mtmh_leaves_mean, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "ax1.set_title(f'Average Leaves per Tree (across {n_runs} runs)')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Average Leaves')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot depths\n",
    "ax2.plot(iterations, default_depths_mean, label='Default BART', alpha=0.7, linewidth=1)\n",
    "ax2.plot(iterations, mtmh_depths_mean, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "ax2.set_title(f'Average Tree Depth (across {n_runs} runs)')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Average Depth')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmas & RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract sigmas and rmses data for visualization\n",
    "n_runs = experiment_results['metadata'].item()['n_runs']\n",
    "\n",
    "fig, axes = plt.subplots(n_runs, 2, figsize=(15, 4*n_runs))\n",
    "if n_runs == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    # Extract data for this run\n",
    "    default_sigmas = experiment_results['default'].item()['sigmas'][run_id]\n",
    "    mtmh_sigmas = experiment_results['mtmh'].item()['sigmas'][run_id]\n",
    "    default_rmses = experiment_results['default'].item()['rmses'][run_id]\n",
    "    mtmh_rmses = experiment_results['mtmh'].item()['rmses'][run_id]\n",
    "\n",
    "    # Plot sigmas (left column)\n",
    "    axes[run_id, 0].plot(default_sigmas, label='Default BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 0].plot(mtmh_sigmas, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 0].set_title(f'Run {run_id} - Sigmas')\n",
    "    axes[run_id, 0].set_xlabel('Iteration')\n",
    "    axes[run_id, 0].set_ylabel('Sigmas')\n",
    "    axes[run_id, 0].legend()\n",
    "    axes[run_id, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot rmses (right column)\n",
    "    axes[run_id, 1].plot(default_rmses, label='Default BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 1].plot(mtmh_rmses, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 1].set_title(f'Run {run_id} - RMSEs')\n",
    "    axes[run_id, 1].set_xlabel('Iteration')\n",
    "    axes[run_id, 1].set_ylabel('RMSE')\n",
    "    axes[run_id, 1].legend()\n",
    "    axes[run_id, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract sigmas and rmses data for visualization\n",
    "n_runs = experiment_results['metadata'].item()['n_runs']\n",
    "\n",
    "fig, axes = plt.subplots(n_runs, 2, figsize=(15, 4*n_runs))\n",
    "if n_runs == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    # Extract data for this run\n",
    "    default_sigmas = experiment_results['default'].item()['sigmas'][run_id]\n",
    "    mtmh_sigmas = experiment_results['mtmh'].item()['sigmas'][run_id]\n",
    "    default_rmses = experiment_results['default'].item()['rmses'][run_id]\n",
    "    mtmh_rmses = experiment_results['mtmh'].item()['rmses'][run_id]\n",
    "\n",
    "    # Plot sigmas (left column)\n",
    "    axes[run_id, 0].plot(default_sigmas[3000:], label='Default BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 0].plot(mtmh_sigmas[3000:], label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 0].set_title(f'Run {run_id} - Sigmas')\n",
    "    axes[run_id, 0].set_xlabel('Iteration')\n",
    "    axes[run_id, 0].set_ylabel('Sigmas')\n",
    "    axes[run_id, 0].legend()\n",
    "    axes[run_id, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot rmses (right column)\n",
    "    axes[run_id, 1].plot(default_rmses[3000:], label='Default BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 1].plot(mtmh_rmses[3000:], label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "    axes[run_id, 1].set_title(f'Run {run_id} - RMSEs')\n",
    "    axes[run_id, 1].set_xlabel('Iteration')\n",
    "    axes[run_id, 1].set_ylabel('RMSE')\n",
    "    axes[run_id, 1].legend()\n",
    "    axes[run_id, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract data across all runs\n",
    "n_runs = experiment_results['metadata'].item()['n_runs']\n",
    "\n",
    "# Collect all runs data\n",
    "default_sigmas_all = []\n",
    "mtmh_sigmas_all = []\n",
    "default_rmses_all = []\n",
    "mtmh_rmses_all = []\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    default_sigmas_all.append(experiment_results['default'].item()['sigmas'][run_id])\n",
    "    mtmh_sigmas_all.append(experiment_results['mtmh'].item()['sigmas'][run_id])\n",
    "    default_rmses_all.append(experiment_results['default'].item()['rmses'][run_id])\n",
    "    mtmh_rmses_all.append(experiment_results['mtmh'].item()['rmses'][run_id])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "default_sigmas_all = np.array(default_sigmas_all)  # Shape: (n_runs, n_iterations)\n",
    "mtmh_sigmas_all = np.array(mtmh_sigmas_all)\n",
    "default_rmses_all = np.array(default_rmses_all)\n",
    "mtmh_rmses_all = np.array(mtmh_rmses_all)\n",
    "\n",
    "# Calculate mean and std across runs\n",
    "default_sigmas_mean = np.mean(default_sigmas_all, axis=0)\n",
    "mtmh_sigmas_mean = np.mean(mtmh_sigmas_all, axis=0)\n",
    "\n",
    "default_rmses_mean = np.mean(default_rmses_all, axis=0)\n",
    "mtmh_rmses_mean = np.mean(mtmh_rmses_all, axis=0)\n",
    "\n",
    "# Plot averaged results with error bands\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot sigmas\n",
    "iterations = np.arange(len(default_sigmas_mean))\n",
    "ax1.plot(iterations, default_sigmas_mean, label='Default BART', alpha=0.7, linewidth=1)\n",
    "ax1.plot(iterations, mtmh_sigmas_mean, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "ax1.set_title(f'Sigmas (across {n_runs} runs)')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Sigmas')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot rmses\n",
    "iterations = np.arange(len(default_rmses_mean))\n",
    "ax2.plot(iterations, default_rmses_mean, label='Default BART', alpha=0.7, linewidth=1)\n",
    "ax2.plot(iterations, mtmh_rmses_mean, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "ax2.set_title(f'RMSE (across {n_runs} runs)')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data after burn-in\n",
    "default_sigmas_mean = default_sigmas_mean[3000:]\n",
    "mtmh_sigmas_mean = mtmh_sigmas_mean[3000:]\n",
    "\n",
    "default_rmses_mean = default_rmses_mean[3000:]\n",
    "mtmh_rmses_mean = mtmh_rmses_mean[3000:]\n",
    "\n",
    "# Plot averaged results with error bands\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot sigmas\n",
    "iterations = np.arange(len(default_sigmas_mean))\n",
    "ax1.plot(iterations, default_sigmas_mean, label='Default BART', alpha=0.7, linewidth=1)\n",
    "ax1.plot(iterations, mtmh_sigmas_mean, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "ax1.set_title(f'Sigmas (across {n_runs} runs)')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Sigmas')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot rmses\n",
    "iterations = np.arange(len(default_rmses_mean))\n",
    "ax2.plot(iterations, default_rmses_mean, label='Default BART', alpha=0.7, linewidth=1)\n",
    "ax2.plot(iterations, mtmh_rmses_mean, label='MTMH BART', alpha=0.7, linewidth=1)\n",
    "ax2.set_title(f'RMSE (across {n_runs} runs)')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Analysis\n",
    "## KPSS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagnosis import segmented_kpss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each run's results and collect statistics\n",
    "n_runs = experiment_results['metadata'].item()['n_runs']\n",
    "\n",
    "# Collect convergence statistics\n",
    "default_sigma_convergence = []\n",
    "default_rmse_convergence = []\n",
    "default_sigma_rates = []\n",
    "default_rmse_rates = []\n",
    "\n",
    "# Analyze Default BART results\n",
    "print(\"=== Default BART Analysis ===\")\n",
    "for run_id in range(n_runs):\n",
    "    print(f\"\\n--- Default Run {run_id} ---\")\n",
    "    \n",
    "    # Extract data for this run from the combined arrays\n",
    "    sigmas = experiment_results['default'].item()['sigmas'][run_id]\n",
    "    rmses = experiment_results['default'].item()['rmses'][run_id]\n",
    "    \n",
    "    print(f\"Sigma convergence analysis:\")\n",
    "    convergence_result = segmented_kpss_test(sigmas, segment_length=100)\n",
    "    print(f\"Default Chain converged: {convergence_result['converged']}\")\n",
    "    if convergence_result['converged']:\n",
    "        print(f\"Convergence at iteration: {convergence_result['convergence_iteration']}\")\n",
    "        default_sigma_convergence.append(convergence_result['convergence_iteration'])\n",
    "    print(f\"Convergence rate: {convergence_result['convergence_rate']:.2%}\")\n",
    "    default_sigma_rates.append(convergence_result['convergence_rate'])\n",
    "    \n",
    "    print(f\"\\nRMSE convergence analysis:\")\n",
    "    convergence_result = segmented_kpss_test(rmses, segment_length=100)\n",
    "    print(f\"Default Chain converged: {convergence_result['converged']}\")\n",
    "    if convergence_result['converged']:\n",
    "        print(f\"Convergence at iteration: {convergence_result['convergence_iteration']}\")\n",
    "        default_rmse_convergence.append(convergence_result['convergence_iteration'])\n",
    "    print(f\"Convergence rate: {convergence_result['convergence_rate']:.2%}\")\n",
    "    default_rmse_rates.append(convergence_result['convergence_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each run's results and collect statistics\n",
    "n_runs = 5\n",
    "\n",
    "# Collect convergence statistics\n",
    "mtmh_sigma_convergence = []\n",
    "mtmh_rmse_convergence = []\n",
    "mtmh_sigma_rates = []\n",
    "mtmh_rmse_rates = []\n",
    "\n",
    "# Analyze MTMH BART results\n",
    "print(\"=== MTMH BART Analysis ===\")\n",
    "for run_id in range(n_runs):\n",
    "    print(f\"\\n--- MTMH Run {run_id} ---\")\n",
    "    \n",
    "    # Extract data for this run from the combined arrays\n",
    "    sigmas = experiment_results['mtmh'].item()['sigmas'][run_id]\n",
    "    rmses = experiment_results['mtmh'].item()['rmses'][run_id]\n",
    "\n",
    "    print(f\"Sigma convergence analysis:\")\n",
    "    convergence_result = segmented_kpss_test(sigmas, segment_length=100)\n",
    "    print(f\"MTMH Chain converged: {convergence_result['converged']}\")\n",
    "    if convergence_result['converged']:\n",
    "        print(f\"Convergence at iteration: {convergence_result['convergence_iteration']}\")\n",
    "        mtmh_sigma_convergence.append(convergence_result['convergence_iteration'])\n",
    "    print(f\"Convergence rate: {convergence_result['convergence_rate']:.2%}\")\n",
    "    mtmh_sigma_rates.append(convergence_result['convergence_rate'])\n",
    "\n",
    "    print(f\"\\nRMSE convergence analysis:\")\n",
    "    convergence_result = segmented_kpss_test(rmses, segment_length=100)\n",
    "    print(f\"MTMH Chain converged: {convergence_result['converged']}\")\n",
    "    if convergence_result['converged']:\n",
    "        print(f\"Convergence at iteration: {convergence_result['convergence_iteration']}\")\n",
    "        mtmh_rmse_convergence.append(convergence_result['convergence_iteration'])\n",
    "    print(f\"Convergence rate: {convergence_result['convergence_rate']:.2%}\")\n",
    "    mtmh_rmse_rates.append(convergence_result['convergence_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics for Default BART\n",
    "print(\"\\n=== Default BART Summary ===\")\n",
    "if default_sigma_convergence:\n",
    "    print(f\"Sigma - Average convergence iteration: {np.mean(default_sigma_convergence):.0f}\")\n",
    "else:\n",
    "    print(\"Sigma - No convergence detected\")\n",
    "print(f\"Sigma - Average convergence rate: {np.mean(default_sigma_rates):.2%}\")\n",
    "\n",
    "if default_rmse_convergence:\n",
    "    print(f\"RMSE - Average convergence iteration: {np.mean(default_rmse_convergence):.0f}\")\n",
    "else:\n",
    "    print(\"RMSE - No convergence detected\")\n",
    "print(f\"RMSE - Average convergence rate: {np.mean(default_rmse_rates):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics for MTMH BART\n",
    "print(\"\\n=== MTMH BART Summary ===\")\n",
    "if mtmh_sigma_convergence:\n",
    "    print(f\"Sigma - Average convergence iteration: {np.mean(mtmh_sigma_convergence):.0f}\")\n",
    "else:\n",
    "    print(\"Sigma - No convergence detected\")\n",
    "print(f\"Sigma - Average convergence rate: {np.mean(mtmh_sigma_rates):.2%}\")\n",
    "\n",
    "if mtmh_rmse_convergence:\n",
    "    print(f\"RMSE - Average convergence iteration: {np.mean(mtmh_rmse_convergence):.0f}\")\n",
    "else:\n",
    "    print(\"RMSE - No convergence detected\")\n",
    "print(f\"RMSE - Average convergence rate: {np.mean(mtmh_rmse_rates):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logging configuration before importing arviz\n",
    "import logging\n",
    "logging.getLogger('arviz.preview').setLevel(logging.WARNING)\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract prediction data for ESS analysis\n",
    "n_runs = experiment_results['metadata'].item()['n_runs']\n",
    "\n",
    "# Collect ESS values for each test data point across all runs\n",
    "default_ess_per_point = []\n",
    "mtmh_ess_per_point = []\n",
    "\n",
    "print(\"=== Prediction ESS Analysis ===\")\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    print(f\"\\n--- Run {run_id} ---\")\n",
    "    \n",
    "    # Extract prediction data for this run (shape: n_test_samples x n_iterations)\n",
    "    default_preds = experiment_results['default'].item()['preds'][run_id]\n",
    "    mtmh_preds = experiment_results['mtmh'].item()['preds'][run_id]\n",
    "    \n",
    "    # Remove burn-in period (first 3000 iterations)\n",
    "    default_preds_burnin = default_preds[:, 3000:]  # Shape: n_test_samples x (n_iterations - 3000)\n",
    "    mtmh_preds_burnin = mtmh_preds[:, 3000:]\n",
    "    \n",
    "    # Calculate ESS for each test data point\n",
    "    default_ess_run = []\n",
    "    mtmh_ess_run = []\n",
    "    \n",
    "    n_test_samples = default_preds_burnin.shape[0]\n",
    "    \n",
    "    for i in range(n_test_samples):\n",
    "        # ESS for each test point's prediction trace\n",
    "        default_ess = az.ess(default_preds_burnin[i].reshape(1, -1), relative=True).item()\n",
    "        mtmh_ess = az.ess(mtmh_preds_burnin[i].reshape(1, -1), relative=True).item()\n",
    "        \n",
    "        default_ess_run.append(default_ess)\n",
    "        mtmh_ess_run.append(mtmh_ess)\n",
    "    \n",
    "    default_ess_per_point.append(default_ess_run)\n",
    "    mtmh_ess_per_point.append(mtmh_ess_run)\n",
    "    \n",
    "    print(f\"Default BART - Mean ESS: {np.mean(default_ess_run):.4f}, Std ESS: {np.std(default_ess_run):.4f}\")\n",
    "    print(f\"MTMH BART - Mean ESS: {np.mean(mtmh_ess_run):.4f}, Std ESS: {np.std(mtmh_ess_run):.4f}\")\n",
    "\n",
    "# Convert to numpy arrays for easier manipulation\n",
    "default_ess_per_point = np.array(default_ess_per_point)  # Shape: (n_runs, n_test_samples)\n",
    "mtmh_ess_per_point = np.array(mtmh_ess_per_point)\n",
    "\n",
    "print(f\"\\nOverall across all runs:\")\n",
    "print(f\"Default BART - Mean ESS: {np.mean(default_ess_per_point):.4f}, Std ESS: {np.std(default_ess_per_point):.4f}\")\n",
    "print(f\"MTMH BART - Mean ESS: {np.mean(mtmh_ess_per_point):.4f}, Std ESS: {np.std(mtmh_ess_per_point):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ESS comparison for each test data point using scatter plots\n",
    "fig, axes = plt.subplots(n_runs, 1, figsize=(15, 4*n_runs))\n",
    "if n_runs == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for run_id in range(n_runs):\n",
    "    x = np.arange(len(default_ess_per_point[run_id]))\n",
    "    \n",
    "    axes[run_id].scatter(x, default_ess_per_point[run_id], \n",
    "                        label='Default BART', alpha=0.7, s=10)\n",
    "    axes[run_id].scatter(x, mtmh_ess_per_point[run_id], \n",
    "                        label='MTMH BART', alpha=0.7, s=10)\n",
    "\n",
    "    axes[run_id].set_title(f'Run {run_id} - ESS per Test Data Point')\n",
    "    axes[run_id].set_xlabel('Test Data Point Index')\n",
    "    axes[run_id].set_ylabel('ESS (Relative)')\n",
    "    axes[run_id].legend()\n",
    "    axes[run_id].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot averaged ESS across all runs (simplified without error bars)\n",
    "mean_default_ess = np.mean(default_ess_per_point, axis=0)\n",
    "mean_mtmh_ess = np.mean(mtmh_ess_per_point, axis=0)\n",
    "\n",
    "x = np.arange(len(mean_default_ess))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "\n",
    "ax.scatter(x, mean_default_ess, label='Default BART', alpha=0.7, s=10)\n",
    "ax.scatter(x, mean_mtmh_ess, label='MTMH BART', alpha=0.7, s=10)\n",
    "\n",
    "ax.set_title(f'Average ESS per Test Data Point (across {n_runs} runs)')\n",
    "ax.set_xlabel('Test Data Point Index')\n",
    "ax.set_ylabel('ESS (Relative)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"Default BART - Average ESS across all points and runs: {np.mean(default_ess_per_point):.4f}\")\n",
    "print(f\"MTMH BART - Average ESS across all points and runs: {np.mean(mtmh_ess_per_point):.4f}\")\n",
    "print(f\"Improvement ratio (MTMH/Default): {np.mean(mtmh_ess_per_point)/np.mean(default_ess_per_point):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram comparison of ESS values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Flatten all ESS values for histogram\n",
    "default_ess_flat = default_ess_per_point.flatten()\n",
    "mtmh_ess_flat = mtmh_ess_per_point.flatten()\n",
    "\n",
    "# Histogram for Default BART\n",
    "ax1.hist(default_ess_flat, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax1.set_title('Default BART - ESS Distribution')\n",
    "ax1.set_xlabel('ESS (Relative)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(np.mean(default_ess_flat), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(default_ess_flat):.4f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram for MTMH BART\n",
    "ax2.hist(mtmh_ess_flat, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "ax2.set_title('MTMH BART - ESS Distribution')\n",
    "ax2.set_xlabel('ESS (Relative)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(np.mean(mtmh_ess_flat), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(mtmh_ess_flat):.4f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigma & RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each run's results\n",
    "n_runs = experiment_results['metadata'].item()['n_runs']\n",
    "\n",
    "# Analyze Default BART results\n",
    "print(\"=== Default BART Analysis ===\")\n",
    "for run_id in range(n_runs):\n",
    "    print(f\"\\n--- Run {run_id} ---\")\n",
    "    \n",
    "    # Extract data for this run\n",
    "    sigmas = experiment_results['default'].item()['sigmas'][run_id]\n",
    "    rmses = experiment_results['default'].item()['rmses'][run_id]\n",
    "\n",
    "    print(f\"Sigma ess value: {az.ess(sigmas[3000:].reshape(1, -1), relative=True).item():.6f}\")\n",
    "    print(f\"RMSE ess value: {az.ess(rmses[3000:].reshape(1, -1), relative=True).item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each run's results\n",
    "n_runs = 5\n",
    "\n",
    "# Analyze MTMH BART results\n",
    "print(\"=== MTMH BART Analysis ===\")\n",
    "for run_id in range(n_runs):\n",
    "    print(f\"\\n--- Run {run_id} ---\")\n",
    "    \n",
    "    # Extract data for this run from the combined arrays\n",
    "    sigmas = experiment_results['mtmh'].item()['sigmas'][run_id]\n",
    "    rmses = experiment_results['mtmh'].item()['rmses'][run_id]\n",
    "\n",
    "    print(f\"Sigma ess value: {az.ess(sigmas[3000:].reshape(1, -1), relative=True).item():.6f}\")\n",
    "    print(f\"RMSE ess value: {az.ess(rmses[3000:].reshape(1, -1), relative=True).item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagnosis import plot_autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each run's results\n",
    "n_runs = 5\n",
    "\n",
    "# Analyze Default BART results\n",
    "print(\"=== Default BART Analysis ===\")\n",
    "for run_id in range(n_runs):\n",
    "    print(f\"\\n--- Run {run_id} ---\")\n",
    "    \n",
    "    # Extract data for this run\n",
    "    sigmas = experiment_results['default'].item()['sigmas'][run_id]\n",
    "    rmses = experiment_results['default'].item()['rmses'][run_id]\n",
    "\n",
    "    print(f\"Default Sigma autocorrelation plot:\")\n",
    "    plot_autocorrelation(sigmas[3000:], nlags=500)\n",
    "\n",
    "    print(f\"Default RMSE autocorrelation plot:\")\n",
    "    plot_autocorrelation(rmses[3000:], nlags=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each run's results\n",
    "n_runs = 5\n",
    "\n",
    "# Analyze MTMH BART results\n",
    "print(\"=== MTMH BART Analysis ===\")\n",
    "for run_id in range(n_runs):\n",
    "    print(f\"\\n--- Run {run_id} ---\")\n",
    "    \n",
    "    # Extract data for this run from the combined arrays\n",
    "    sigmas = experiment_results['mtmh'].item()['sigmas'][run_id]\n",
    "    rmses = experiment_results['mtmh'].item()['rmses'][run_id]\n",
    "\n",
    "    print(f\"MTMH Sigma autocorrelation plot:\")\n",
    "    plot_autocorrelation(sigmas[3000:], nlags=500)\n",
    "\n",
    "    print(f\"MTMH RMSE autocorrelation plot:\")\n",
    "    plot_autocorrelation(rmses[3000:], nlags=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
